{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Final-Project-Submission\" data-toc-modified-id=\"Final-Project-Submission-0.1\"><span class=\"toc-item-num\">0.1&nbsp;&nbsp;</span>Final Project Submission</a></span></li></ul></li><li><span><a href=\"#data-scrubbing-and-cleaning\" data-toc-modified-id=\"data-scrubbing-and-cleaning-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>data scrubbing and cleaning</a></span><ul class=\"toc-item\"><li><span><a href=\"#import-modules-and-data\" data-toc-modified-id=\"import-modules-and-data-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>import modules and data</a></span></li><li><span><a href=\"#functions\" data-toc-modified-id=\"functions-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>functions</a></span></li><li><span><a href=\"#create-series-to-present-price-in-readable-way-for-visualizations\" data-toc-modified-id=\"create-series-to-present-price-in-readable-way-for-visualizations-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>create series to present price in readable way for visualizations</a></span></li><li><span><a href=\"#fill-in-missing-values-(NaN)-where-applicable\" data-toc-modified-id=\"fill-in-missing-values-(NaN)-where-applicable-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>fill in missing values (NaN) where applicable</a></span></li><li><span><a href=\"#assess-whether-renovation-has-direct-impact-on-home-price\" data-toc-modified-id=\"assess-whether-renovation-has-direct-impact-on-home-price-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>assess whether renovation has direct impact on home price</a></span></li><li><span><a href=\"#convert-date-column-to-DateTime-format,-then-separate-date-to-month,-day,-year-columns-due-to-datetime-being-unusable-in-regression,-(drop-'data'-column-once-all-necessary-calculations-have-been-done)\" data-toc-modified-id=\"convert-date-column-to-DateTime-format,-then-separate-date-to-month,-day,-year-columns-due-to-datetime-being-unusable-in-regression,-(drop-'data'-column-once-all-necessary-calculations-have-been-done)-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>convert date column to DateTime format, then separate date to month, day, year columns due to datetime being unusable in regression, (drop 'data' column once all necessary calculations have been done)</a></span></li><li><span><a href=\"#create-age-column-from-date-of-sale-and-year-built\" data-toc-modified-id=\"create-age-column-from-date-of-sale-and-year-built-1.7\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>create age column from date of sale and year built</a></span><ul class=\"toc-item\"><li><span><a href=\"#drop-columns-not-to-be-utilized\" data-toc-modified-id=\"drop-columns-not-to-be-utilized-1.7.1\"><span class=\"toc-item-num\">1.7.1&nbsp;&nbsp;</span>drop columns not to be utilized</a></span></li></ul></li><li><span><a href=\"#sqft_basement-column-contains-string-characters,-remove-these-and-convert-to-int-to-match-other-similar-columns\" data-toc-modified-id=\"sqft_basement-column-contains-string-characters,-remove-these-and-convert-to-int-to-match-other-similar-columns-1.8\"><span class=\"toc-item-num\">1.8&nbsp;&nbsp;</span>sqft_basement column contains string characters, remove these and convert to int to match other similar columns</a></span></li><li><span><a href=\"#convert-sq_ft-areas-to-square-meters\" data-toc-modified-id=\"convert-sq_ft-areas-to-square-meters-1.9\"><span class=\"toc-item-num\">1.9&nbsp;&nbsp;</span>convert sq_ft areas to square meters</a></span></li><li><span><a href=\"#bedrooms-outlier-present-as-33-bedroom-house,-appears-to-be-typo-due-to-other-info,-should-be-3-bedrooms-as-this-listing-only-has-1.75-bathrooms\" data-toc-modified-id=\"bedrooms-outlier-present-as-33-bedroom-house,-appears-to-be-typo-due-to-other-info,-should-be-3-bedrooms-as-this-listing-only-has-1.75-bathrooms-1.10\"><span class=\"toc-item-num\">1.10&nbsp;&nbsp;</span>bedrooms outlier present as 33 bedroom house, appears to be typo due to other info, should be 3 bedrooms as this listing only has 1.75 bathrooms</a></span></li><li><span><a href=\"#assessing-grade-category\" data-toc-modified-id=\"assessing-grade-category-1.11\"><span class=\"toc-item-num\">1.11&nbsp;&nbsp;</span>assessing grade category</a></span></li><li><span><a href=\"#use-latitude-and-longitude-in-existing-dataset-to-get-the-distance-from-home-to-Seattle(closest-major-city)\" data-toc-modified-id=\"use-latitude-and-longitude-in-existing-dataset-to-get-the-distance-from-home-to-Seattle(closest-major-city)-1.12\"><span class=\"toc-item-num\">1.12&nbsp;&nbsp;</span>use latitude and longitude in existing dataset to get the distance from home to Seattle(closest major city)</a></span><ul class=\"toc-item\"><li><span><a href=\"#import-geopy-modules\" data-toc-modified-id=\"import-geopy-modules-1.12.1\"><span class=\"toc-item-num\">1.12.1&nbsp;&nbsp;</span>import geopy modules</a></span></li><li><span><a href=\"#use-geopy-to-set-up-points-and-calculate-distance\" data-toc-modified-id=\"use-geopy-to-set-up-points-and-calculate-distance-1.12.2\"><span class=\"toc-item-num\">1.12.2&nbsp;&nbsp;</span>use geopy to set up points and calculate distance</a></span></li><li><span><a href=\"#dropping-columns\" data-toc-modified-id=\"dropping-columns-1.12.3\"><span class=\"toc-item-num\">1.12.3&nbsp;&nbsp;</span>dropping columns</a></span></li></ul></li><li><span><a href=\"#dropping-all-sqft-columns-due-to-conversion-to-metric-system\" data-toc-modified-id=\"dropping-all-sqft-columns-due-to-conversion-to-metric-system-1.13\"><span class=\"toc-item-num\">1.13&nbsp;&nbsp;</span>dropping all sqft columns due to conversion to metric system</a></span></li><li><span><a href=\"#create-id-dataframe-to-attach-once-model-is-complete.-remove-id-from-dataset-as-it-isn't-relevant-but-just-identifies-homes\" data-toc-modified-id=\"create-id-dataframe-to-attach-once-model-is-complete.-remove-id-from-dataset-as-it-isn't-relevant-but-just-identifies-homes-1.14\"><span class=\"toc-item-num\">1.14&nbsp;&nbsp;</span>create id dataframe to attach once model is complete. remove id from dataset as it isn't relevant but just identifies homes</a></span></li></ul></li><li><span><a href=\"#check-for-multicollinearity\" data-toc-modified-id=\"check-for-multicollinearity-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>check for multicollinearity</a></span><ul class=\"toc-item\"><li><span><a href=\"#heatmap-for-correlation-assessment\" data-toc-modified-id=\"heatmap-for-correlation-assessment-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>heatmap for correlation assessment</a></span></li><li><span><a href=\"#check-price-skew,-visualize-tranformed-data,-normalize-price-using-log-transformation\" data-toc-modified-id=\"check-price-skew,-visualize-tranformed-data,-normalize-price-using-log-transformation-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>check price skew, visualize tranformed data, normalize price using log transformation</a></span></li></ul></li><li><span><a href=\"#one-hot-encoding,-create-new-df:-dummies\" data-toc-modified-id=\"one-hot-encoding,-create-new-df:-dummies-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>one hot encoding, create new df: dummies</a></span><ul class=\"toc-item\"><li><span><a href=\"#create-dummy-df\" data-toc-modified-id=\"create-dummy-df-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>create dummy df</a></span></li></ul></li><li><span><a href=\"#models\" data-toc-modified-id=\"models-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>models</a></span><ul class=\"toc-item\"><li><span><a href=\"#import-libraries\" data-toc-modified-id=\"import-libraries-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>import libraries</a></span><ul class=\"toc-item\"><li><span><a href=\"#no-dummies-dataframe\" data-toc-modified-id=\"no-dummies-dataframe-4.1.1\"><span class=\"toc-item-num\">4.1.1&nbsp;&nbsp;</span>no dummies dataframe</a></span></li><li><span><a href=\"#df-with-dummies\" data-toc-modified-id=\"df-with-dummies-4.1.2\"><span class=\"toc-item-num\">4.1.2&nbsp;&nbsp;</span>df with dummies</a></span></li></ul></li></ul></li><li><span><a href=\"#single-variable-linear-regression-on-data-with-non-transformed-price\" data-toc-modified-id=\"single-variable-linear-regression-on-data-with-non-transformed-price-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>single variable linear regression on data with non transformed price</a></span></li><li><span><a href=\"#single-variable-linear-regression-on-data-with-log-transformed-price\" data-toc-modified-id=\"single-variable-linear-regression-on-data-with-log-transformed-price-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>single variable linear regression on data with log transformed price</a></span><ul class=\"toc-item\"><li><span><a href=\"#assessing-grade-affect-on-price,-converting-grade-to-grade/13-as-there-are-only-13-possible-grades\" data-toc-modified-id=\"assessing-grade-affect-on-price,-converting-grade-to-grade/13-as-there-are-only-13-possible-grades-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>assessing grade affect on price, converting grade to grade/13 as there are only 13 possible grades</a></span></li></ul></li><li><span><a href=\"#build-model-with-df\" data-toc-modified-id=\"build-model-with-df-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>build model with df</a></span><ul class=\"toc-item\"><li><span><a href=\"#where-X=-data,-with-dummies,-y-=price\" data-toc-modified-id=\"where-X=-data,-with-dummies,-y-=price-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>where X= data, with dummies, y =price</a></span></li><li><span><a href=\"#where-X=data,-y-=-price,-no-dummies\" data-toc-modified-id=\"where-X=data,-y-=-price,-no-dummies-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>where X=data, y = price, no dummies</a></span></li><li><span><a href=\"#regression-on-datasets-with-dummies\" data-toc-modified-id=\"regression-on-datasets-with-dummies-7.3\"><span class=\"toc-item-num\">7.3&nbsp;&nbsp;</span>regression on datasets with dummies</a></span></li><li><span><a href=\"#where-X=-minmax-scaling-applied-to-dataset,-y=log-transformed-price\" data-toc-modified-id=\"where-X=-minmax-scaling-applied-to-dataset,-y=log-transformed-price-7.4\"><span class=\"toc-item-num\">7.4&nbsp;&nbsp;</span>where X= minmax scaling applied to dataset, y=log transformed price</a></span></li><li><span><a href=\"#model-where-X=-data,-no-scaling-applied-to-continuous/y-=-log-transformed-price\" data-toc-modified-id=\"model-where-X=-data,-no-scaling-applied-to-continuous/y-=-log-transformed-price-7.5\"><span class=\"toc-item-num\">7.5&nbsp;&nbsp;</span>model where X= data, no scaling applied to continuous/y = log transformed price</a></span><ul class=\"toc-item\"><li><span><a href=\"#same-as-above-dropping-sqm_lot\" data-toc-modified-id=\"same-as-above-dropping-sqm_lot-7.5.1\"><span class=\"toc-item-num\">7.5.1&nbsp;&nbsp;</span>same as above dropping sqm_lot</a></span></li></ul></li></ul></li><li><span><a href=\"#final-model-assessment\" data-toc-modified-id=\"final-model-assessment-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>final model assessment</a></span><ul class=\"toc-item\"><li><span><a href=\"#using-model-with-no-scaling,---with-target-=-price/10000,-log-transformed-price,-dropping-sqm_lot\" data-toc-modified-id=\"using-model-with-no-scaling,---with-target-=-price/10000,-log-transformed-price,-dropping-sqm_lot-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span>using model with no scaling,   with target = price/10000, log transformed price, dropping sqm_lot</a></span></li><li><span><a href=\"#y-=-log-transformed-price(og)\" data-toc-modified-id=\"y-=-log-transformed-price(og)-8.2\"><span class=\"toc-item-num\">8.2&nbsp;&nbsp;</span>y = log transformed price(og)</a></span></li><li><span><a href=\"#model-with-price-/10000-log-transformed\" data-toc-modified-id=\"model-with-price-/10000-log-transformed-8.3\"><span class=\"toc-item-num\">8.3&nbsp;&nbsp;</span>model with price /10000 log transformed</a></span></li><li><span><a href=\"#model-with-price-log\" data-toc-modified-id=\"model-with-price-log-8.4\"><span class=\"toc-item-num\">8.4&nbsp;&nbsp;</span>model with price log</a></span></li><li><span><a href=\"#breuschpagan-test\" data-toc-modified-id=\"breuschpagan-test-8.5\"><span class=\"toc-item-num\">8.5&nbsp;&nbsp;</span>breuschpagan test</a></span></li><li><span><a href=\"#residuals\" data-toc-modified-id=\"residuals-8.6\"><span class=\"toc-item-num\">8.6&nbsp;&nbsp;</span>residuals</a></span></li><li><span><a href=\"#train/test-split-data\" data-toc-modified-id=\"train/test-split-data-8.7\"><span class=\"toc-item-num\">8.7&nbsp;&nbsp;</span>train/test split data</a></span><ul class=\"toc-item\"><li><span><a href=\"#visualize-training-data-vs-test-data\" data-toc-modified-id=\"visualize-training-data-vs-test-data-8.7.1\"><span class=\"toc-item-num\">8.7.1&nbsp;&nbsp;</span>visualize training data vs test data</a></span></li></ul></li></ul></li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Conclusion</a></span><ul class=\"toc-item\"><li><span><a href=\"#Feature-Engineering-and-Feature-Selection\" data-toc-modified-id=\"Feature-Engineering-and-Feature-Selection-9.1\"><span class=\"toc-item-num\">9.1&nbsp;&nbsp;</span>Feature Engineering and Feature Selection</a></span></li></ul></li><li><span><a href=\"#Future-Assessments\" data-toc-modified-id=\"Future-Assessments-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Future Assessments</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Project Submission\n",
    "\n",
    "Please fill out:\n",
    "* Student name: Brennan Mathis\n",
    "* Student pace: part time\n",
    "* Scheduled project review date/time: 5/20 16:00 EST\n",
    "* Instructor name: Victor G/ Eli\n",
    "* Blog post URL: https://datainsights.data.blog/2020/05/18/analysis-regression-king-county-housing-dataset/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "H0 = features of a house have no effect on price/value of house\n",
    "Ha = features of a house have an effect on price/value of house"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data scrubbing and cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import modules and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"kc_house_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize values\n",
    "def norm_feat(series):\n",
    "    return (series - series.mean())/series.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot encode return dummies\n",
    "def one_hot(series):\n",
    "    return (pd.get_dummies(series, prefix=series.name, drop_first=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform log transformation\n",
    "def get_log(series):\n",
    "    return np.log(series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In metric terms a square foot is a square with sides 0.3048 metres in length. \n",
    "One square foot is the equivalent to 0.09290304 square metres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 sq ft = 0.092903 sq m\n",
    "def ft_to_meter(x):\n",
    "    return x * 0.092903"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step wise selection to assess p values\n",
    "def stepwise_selection(X, y, \n",
    "                       initial_list=[], \n",
    "                       threshold_in=0.01, \n",
    "                       threshold_out = 0.05, \n",
    "                       verbose=True):\n",
    "    \"\"\" Perform a forward-backward feature selection \n",
    "    based on p-value from statsmodels.api.OLS\n",
    "    Arguments:\n",
    "        X - pandas.DataFrame with candidate features\n",
    "        y - list-like with the target\n",
    "        initial_list - list of features to start with (column names of X)\n",
    "        threshold_in - include a feature if its p-value < threshold_in\n",
    "        threshold_out - exclude a feature if its p-value > threshold_out\n",
    "        verbose - whether to print the sequence of inclusions and exclusions\n",
    "    Returns: list of selected features \n",
    "    Always set threshold_in < threshold_out to avoid infinite looping.\n",
    "    See https://en.wikipedia.org/wiki/Stepwise_regression for the details\n",
    "    \"\"\"\n",
    "    included = list(initial_list)\n",
    "    while True:\n",
    "        changed=False\n",
    "        # forward step\n",
    "        excluded = list(set(X.columns)-set(included))\n",
    "        new_pval = pd.Series(index=excluded)\n",
    "        for new_column in excluded:\n",
    "            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n",
    "            new_pval[new_column] = model.pvalues[new_column]\n",
    "        best_pval = new_pval.min()\n",
    "        if best_pval < threshold_in:\n",
    "            best_feature = new_pval.idxmin()\n",
    "            included.append(best_feature)\n",
    "            changed=True\n",
    "            if verbose:\n",
    "                print('Add  {:30} with p-value {:.6}'.format(best_feature, best_pval))\n",
    "\n",
    "        # backward step\n",
    "        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n",
    "        # use all coefs except intercept\n",
    "        pvalues = model.pvalues.iloc[1:]\n",
    "        worst_pval = pvalues.max() # null if pvalues is empty\n",
    "        if worst_pval > threshold_out:\n",
    "            changed=True\n",
    "            worst_feature = pvalues.argmax()\n",
    "            included.remove(worst_feature)\n",
    "            if verbose:\n",
    "                print('Drop {:30} with p-value {:.6}'.format(worst_feature, worst_pval))\n",
    "        if not changed:\n",
    "            break\n",
    "    return included"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create series to present price in readable way for visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create series to present price in readable way for visualizations\n",
    "data['price/10000'] = data['price']/10000\n",
    "data['price/1000'] = data['price']/1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fill in missing values (NaN) where applicable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill nan values\n",
    "data.waterfront.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['waterfront'] = data.waterfront.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to int\n",
    "data['waterfront'] = data['waterfront'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill nan values\n",
    "data.yr_renovated = data.yr_renovated.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## assess whether renovation has direct impact on home price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.yr_renovated = data.yr_renovated.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#binary response for whether renovated or not\n",
    "data.yr_renovated = data.yr_renovated.apply(lambda row: 1 if (row > 0) else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convert date column to DateTime format, then separate date to month, day, year columns due to datetime being unusable in regression, (drop 'data' column once all necessary calculations have been done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['date']= pd.to_datetime(data['date']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['sale_year'] = data.date.dt.year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create age column from date of sale and year built "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subtract the year built from sale year to get age of home when sold\n",
    "data['age'] = data['sale_year'] - (data['yr_built'] - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "renovated_df = data.loc[data['yr_renovated'] > 1]\n",
    "non_reno_df = data.loc[data['yr_renovated'] < 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### drop columns not to be utilized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['yr_renovated', 'waterfront', 'view'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['view'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sqft_basement column contains string characters, remove these and convert to int to match other similar columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to int format, coercing strings to 0\n",
    "data['sqft_basement'] = pd.to_numeric(data['sqft_basement'], \n",
    "                        downcast= 'integer', errors ='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace missing values using data provided for homes without nan values\n",
    "# sqft_basement + sqft_above = sqft_living\n",
    "data['sqft_basement'] = data['sqft_basement'].fillna(data['sqft_living'] \n",
    "                             - data['sqft_above'])\n",
    "data['sqft_basement'] = data['sqft_basement'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this column will probably be dropped due to direct correlation to sqft_living (and sqft_above )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convert sq_ft areas to square meters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ft_to_meters \n",
    "data['sqm_living'] = ft_to_meter(data.sqft_living)\n",
    "data['sqm_living15'] = ft_to_meter(data.sqft_living15)\n",
    "data['sqm_lot'] = ft_to_meter(data.sqft_lot)\n",
    "data['sqm_lot15'] = ft_to_meter(data.sqft_lot15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "omitting sqft_above and sqft_basement due to correlation with sqft_living, and will most likely be dropped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  bedrooms outlier present as 33 bedroom house, appears to be typo due to other info, should be 3 bedrooms as this listing only has 1.75 bathrooms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['bedrooms'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data[\"bedrooms\"] == 33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"bedrooms\"].replace({33: 3}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(style=\"whitegrid\")\n",
    "plt.figure(figsize=(18,10))\n",
    "\n",
    "plt.figure(figsize=(18,10))\n",
    "data['bedrooms'].hist()\n",
    "plt.xlabel('Number of Bedrooms in Home', fontsize=20) \n",
    "plt.ylabel('Count', fontsize=20) \n",
    "plt.title('Distribution of Bedroom Count Per Home', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data = data[data.bathrooms < 6]\n",
    "data = data[data.bedrooms < ]\n",
    "data.bathrooms.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data.bathrooms > 0.75]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## assessing grade category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* from king county website regarding grade category\n",
    "\n",
    "Represents the construction quality of improvements. Grades run from grade 1 to 13. Generally defined as:\n",
    "\n",
    "1-3 Falls short of minimum building standards. Normally cabin or inferior structure.\n",
    "\n",
    "4 Generally older, low quality construction. Does not meet code.\n",
    "\n",
    "5 Low construction costs and workmanship. Small, simple design.\n",
    "\n",
    "6 Lowest grade currently meeting building code. Low quality materials and simple designs.\n",
    "\n",
    "7 Average grade of construction and design. Commonly seen in plats and older sub-divisions.\n",
    "\n",
    "8 Just above average in construction and design. Usually better materials in both the exterior and interior finish work.\n",
    "\n",
    "9 Better architectural design with extra interior and exterior design and quality.\n",
    "\n",
    "10 Homes of this quality generally have high quality features. Finish work is better and more design quality is seen in the floor plans. Generally have a larger square footage.\n",
    "\n",
    "11 Custom design and higher quality finish work with added amenities of solid woods, bathroom fixtures and more luxurious options.\n",
    "\n",
    "12 Custom design and excellent builders. All materials are of the highest quality and all conveniences are present.\n",
    "\n",
    "13 Generally custom designed and built. Mansion level. Large amount of highest quality cabinet work, wood trim, marble, entry ways etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing homes that do not meet code\n",
    "#data = data[data.grade > 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## use latitude and longitude in existing dataset to get the distance from home to Seattle(closest major city)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import geopy modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.distance import geodesic\n",
    "from geopy import Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seattle coordinates\n",
    "seattle_lat = 47.3635\n",
    "seattle_long = -122.1959\n",
    "seattle_point = Point(latitude=seattle_lat, longitude =seattle_long)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use geopy to set up points and calculate distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create points for each home using longitude and latitude\n",
    "data['point'] = data.apply(lambda row: Point(latitude=row['lat'], \n",
    "                                             longitude=row['long']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get distance in km from point to seattle\n",
    "data['distance_seattle'] = data.apply(lambda row: \n",
    "                                      geodesic(row['point'], \n",
    "                                      seattle_point).km, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# round distance to seattle values for simplicity\n",
    "data['distance_seattle'] = data['distance_seattle'].round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dropping columns "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dropping point, lat and long columns, derived data needed from these for distance to seattle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['point', 'lat', 'long'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "drop date column due to derived data needed and this column will just cause issues. also dropping view column, as it just tells if the home has been viewed according to documentation. yr_built and sale_year used for age column and can be dropped as well. # drop condition due to the fact that it is contingent on perspective, whereas \n",
    "the 'grade' category is qualified by king county as an official rating  \n",
    "regarding the home inspection and quality of materials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['date','sale_year', 'yr_built' ], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dropping all sqft columns due to conversion to metric system\n",
    "#dropping correlating columns above and basement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['sqft_living', 'sqft_lot', 'sqft_living15','sqft_above', 'sqft_basement',\n",
    "       'sqft_lot15', 'sqm_lot15'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['sqft_living'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create id dataframe to attach once model is complete. remove id from dataset as it isn't relevant but just identifies homes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_df = data['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check for multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.cov()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "abs(data.corr()) > 0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " HIGHLY CORRELATED VARIABLES OVER 0.75:\n",
    " \n",
    " sqft_living/bathrooms\n",
    " sqft_above/sqft_living\n",
    " bathrooms/sqft_living\n",
    " bathrooms/sqft_above\n",
    " sqft_living/sqft_living15\n",
    " sqft_living/sqft_above\n",
    " sqft_living15/sqft_above\n",
    " sqft_lot/sqft_lot15\n",
    "Consider dropping redundant variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## heatmap for correlation assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(18,12))\n",
    "sns.heatmap(data.corr(), annot = True,  fmt='.2g', vmin=-1, vmax=1, center= 0, \n",
    "            cmap= 'coolwarm', linewidths=1, linecolor='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check price skew, visualize tranformed data, normalize price using log transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_th_log = pd.DataFrame\n",
    "price_th_log = get_log(data['price/1000'])\n",
    "price_th_log.plot.kde()\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print (\"Skew is:\", price_th_log.skew())\n",
    "plt.hist(price_th_log)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_tenth_log = pd.DataFrame\n",
    "price_tenth_log = get_log(data['price/10000'])\n",
    "price_tenth_log.plot.kde()\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print (\"Skew is:\", price_tenth_log.skew())\n",
    "plt.hist(price_tenth_log)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_log= pd.DataFrame\n",
    "price_log = get_log(data.price)\n",
    "price_log.plot.kde()\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print (\"Skew is:\", price_log.skew())\n",
    "plt.hist(price_log)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# one hot encoding, create new df: dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_columns = ['bedrooms', 'bathrooms', 'floors', 'waterfront', 'condition',\n",
    "       'grade', 'zipcode', 'age']\n",
    "dummies = pd.get_dummies(data[relevant_columns], drop_first=True, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#round  bathrooms to simplify\n",
    "#data['bathrooms'] = data['bathrooms'].round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create bedroom dummies\n",
    "dumbed = pd.get_dummies(data['bedrooms'], prefix= 'bed', drop_first=True)\n",
    "\n",
    "#create grade dummies\n",
    "dumgrade = pd.get_dummies(data['grade'], prefix= 'grade', drop_first=True)\n",
    "\n",
    "#create condition dummies\n",
    "con_dum = pd.get_dummies(data['condition'], prefix='condition', drop_first=True)\n",
    "\n",
    "# create bathroom dummies\n",
    "bath_dummy = pd.get_dummies(data['bathrooms'], prefix=\"bathrooms\", drop_first=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create dummy df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies = pd.concat([dumgrade, dumbed, bath_dummy, con_dum], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.stats.api as sms\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats import diagnostic\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import RFE\n",
    "from statsmodels.graphics.gofplots import ProbPlot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### no dummies dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['price/10000',  'price/1000' ], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### df with dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dummies = data.copy()\n",
    "df_dummies.drop(['bedrooms', 'bathrooms', 'floors', 'waterfront', 'condition',\n",
    "       'grade', 'zipcode', 'age'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dummies = pd.concat([df_dummies, dummies,], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## apply minmax scaling to continuous data\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont.drop(['bathrooms', 'bedrooms', 'grade',   'condition',  ], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont[['sqm_living', 'sqm_living15', 'sqm_lot', 'distance_seattle', 'age', 'price']] = scaler.fit_transform(cont[['sqm_living', 'sqm_living', 'sqm_lot', 'distance_seattle', 'age', 'price']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# single variable linear regression on data with non transformed price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [['ind_var', 'r_squared', 'intercept', 'slope', 'p-value', 'normality (JB)' ]]\n",
    "for idx, val in enumerate(['bedrooms', 'bathrooms', 'grade', 'sqm_living15', 'condition', 'age', \n",
    "                           'distance_seattle', 'sqm_living', 'sqm_lot']):\n",
    "    print (\"King County Housing DataSet - Regression Analysis and Diagnostics for formula: price~\" + val)\n",
    "    print (\"-------------------------------------------------------------------------------------\")\n",
    "\n",
    "    f = 'price~' + val\n",
    "   \n",
    "    model = smf.ols(formula=f, data=data).fit()\n",
    "    \n",
    "    X_new = pd.DataFrame({val: [data[val].min(), data[val].max()]});\n",
    "    preds = model.predict(X_new)\n",
    "    data.plot(kind='scatter', x=val, y='price');\n",
    "    plt.plot(X_new, preds, linewidth=2);\n",
    "    plt.show()\n",
    "    fig = plt.figure(figsize=(15,8))\n",
    "    fig = sm.graphics.plot_regress_exog(model, val, fig=fig)\n",
    "    fig = sm.graphics.qqplot(model.resid, dist=stats.norm, line='45', fit=True,   )\n",
    "    plt.show()\n",
    "    display('For 1 unit change in  {}, y is affected by change of {}'.format(val, model.params[1]))\n",
    "    results.append([val, model.rsquared, model.params[0], model.params[1], model.pvalues[1], sms.jarque_bera(model.resid)[0] ])\n",
    "    display(results)\n",
    "    \n",
    "    input(\"Press Enter to continue...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 'price~bedrooms'\n",
    "simple_model = ols(formula=f, data=df).fit()\n",
    "simple_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 'price~bedrooms+bathrooms'\n",
    "simple_model = ols(formula=f, data=df).fit()\n",
    "simple_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 'price~bedrooms+bathrooms+grade'\n",
    "simple_model = ols(formula=f, data=df).fit()\n",
    "simple_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 'price~bedrooms+bathrooms+grade+age'\n",
    "simple_model = ols(formula=f, data=df).fit()\n",
    "simple_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 'price~bedrooms+bathrooms+grade+age+condition'\n",
    "simple_model = ols(formula=f, data=df).fit()\n",
    "simple_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 'price~bedrooms+bathrooms+grade+distance_seattle+age+condition'\n",
    "simple_model = ols(formula=f, data=df).fit()\n",
    "simple_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 'price~sqm_living+bathrooms+grade+sqm_lot+sqm_living15+age+distance_seattle+bedrooms'\n",
    "simple_model = ols(formula=f, data=df).fit()\n",
    "simple_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 'price~sqm_living'\n",
    "simple_model = ols(formula=f, data=df).fit()\n",
    "simple_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# single variable linear regression on data with log transformed price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_plog = data.copy()\n",
    "data_plog['price'] = get_log(data_plog['price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [['ind_var', 'r_squared', 'intercept', 'slope', 'p-value', 'normality (JB)' ]]\n",
    "for idx, val in enumerate(['bedrooms', 'bathrooms', 'grade', 'sqm_living15', 'condition', 'age', \n",
    "                           'distance_seattle', 'sqm_living', 'sqm_lot']):\n",
    "    print (\"King County Housing DataSet - Regression Analysis and Diagnostics for formula: price~\" + val)\n",
    "    print (\"-------------------------------------------------------------------------------------\")\n",
    "\n",
    "    f = 'price~' + val\n",
    "   \n",
    "    model = smf.ols(formula=f, data=data_plog).fit()\n",
    "    \n",
    "    X_new = pd.DataFrame({val: [data[val].min(), data[val].max()]});\n",
    "    preds = model.predict(X_new)\n",
    "    data.plot(kind='scatter', x=val, y='price');\n",
    "    plt.plot(X_new, preds, linewidth=2);\n",
    "    plt.show()\n",
    "    fig = plt.figure(figsize=(15,8))\n",
    "    fig = sm.graphics.plot_regress_exog(model, val, fig=fig)\n",
    "    fig = sm.graphics.qqplot(model.resid, dist=stats.norm, line='45', fit=True,   )\n",
    "    plt.show()\n",
    "    \n",
    "    results.append([val, model.rsquared, model.params[0], model.params[1], model.pvalues[1], sms.jarque_bera(model.resid)[0] ])\n",
    "    display(results)\n",
    "    input(\"Press Enter to continue...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 'price~bedrooms'\n",
    "simple_model = ols(formula=f, data=data_plog).fit()\n",
    "simple_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_plog['bedrooms'] = data_plog['bedrooms']**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# polynomial regression on bedrooms **2\n",
    "f = 'price~bedrooms'\n",
    "simple_model = ols(formula=f, data=data_plog).fit()\n",
    "simple_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_plog = data.copy()\n",
    "data_plog['price'] = get_log(data_plog.price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 'price~sqm_living'\n",
    "simple_model = ols(formula=f, data=data_plog).fit()\n",
    "simple_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "each added square meter of living area increases the price by 0.4%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_plog = data.copy()\n",
    "data_plog.drop(['price'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_plog = pd.concat([data_plog, price_log], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 'price~grade+condition'\n",
    "simple_model = ols(formula=f, data=data_plog).fit()\n",
    "simple_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 'price~age+grade+condition'\n",
    "simple_model = ols(formula=f, data=data_plog).fit()\n",
    "simple_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "condition and grade more influential over price but age is helpful combined with these variables but not on its own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 'price~bathrooms'\n",
    "simple_model = ols(formula=f, data=data_plog).fit()\n",
    "simple_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## assessing grade affect on price, converting grade to grade/13 as there are only 13 possible grades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 'price~grade'\n",
    "simple_model = ols(formula=f, data=data_plog).fit()\n",
    "simple_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "price is affected by 32% increase per unit increase of grade of home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 'price~age'\n",
    "simple_model = ols(formula=f, data=data_plog).fit()\n",
    "simple_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 'price~condition'\n",
    "simple_model = ols(formula=f, data=data_plog).fit()\n",
    "simple_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build model with df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## where X= data, with dummies, y =price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_b = df_dummies.price\n",
    "X_b = df_dummies.copy()\n",
    "X_b.drop(['price',], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X_fin3 = X_b\n",
    "X_with_intercept = sm.add_constant(X_b)\n",
    "model3 = sm.OLS(y_b, X_with_intercept).fit()\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## where X=data, y = price, no dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define target variable 'price'\n",
    "labels = data['price']\n",
    "\n",
    "features = df.copy()\n",
    "features.drop(['price', ], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "X_fin = features\n",
    "X_with_intercept = sm.add_constant(features)\n",
    "model = sm.OLS(labels, X_with_intercept).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = LinearRegression()\n",
    "my_model.fit(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = my_model.predict(features)\n",
    "mse = mean_squared_error(labels, predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_test = my_model.predict(features)\n",
    "predict_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get intercept and coef\n",
    "print('The intercept for model is {:.8}'.format(my_model.intercept_))\n",
    "\n",
    "\n",
    "#loop through dict to print\n",
    "for cf in zip(features.columns, my_model.coef_):\n",
    "    print('The coefficient for {} is {:7}'.format(cf[0], cf[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(\n",
    "            my_model, \n",
    "            features,\n",
    "            labels,\n",
    "            cv=20,\n",
    "            scoring=\"neg_mean_squared_error\"\n",
    ")\n",
    "\n",
    "rmse_scores = np.sqrt(-scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(rmse_scores)\n",
    "\n",
    "print(f'Mean: {rmse_scores.mean()}')\n",
    "print(f'Std Dev: {rmse_scores.std()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data=my_model.coef_.reshape(1,-1), columns= features.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the data clearly needs to be normalized as the coefficients show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossvalidation = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "baseline = np.mean(cross_val_score(my_model, features, labels, scoring='r2', cv=crossvalidation))\n",
    "\n",
    "baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## regression on datasets with dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dummies.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## where X= minmax scaling applied to dataset, y=log transformed price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe with scaling applied to continuous variables\n",
    "df_scale_dummies = df_dummies.copy()\n",
    "df_scale_dummies.drop(['price',  ], axis=1, inplace=True)\n",
    "X_s = scaler.fit_transform(df_scale_dummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create x & y\n",
    "y_s = price_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create model for \n",
    "X_fin = X_s\n",
    "X_with_intercept = sm.add_constant(X_s)\n",
    "model1 = sm.OLS(y_s, X_with_intercept).fit()\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing sqm_lot data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe with scaling applied to continuous variables\n",
    "y_s = get_log(data['price'])\n",
    "df_scale_dummies = df_dummies.copy()\n",
    "df_scale_dummies.drop(['price', 'sqm_lot'], axis=1, inplace=True)\n",
    "X_s = scaler.fit_transform(df_scale_dummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scale_dummies.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fin = X_s\n",
    "X_with_intercept = sm.add_constant(X_s)\n",
    "model1_2 = sm.OLS(y_s, X_with_intercept).fit()\n",
    "model1_2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model where X= data, no scaling applied to continuous/y = log transformed price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dummies.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "X = df_dummies.copy()\n",
    "X.drop(['price', ], axis=1, inplace=True)\n",
    "\n",
    "y = price_log\n",
    "X_with_intercept = sm.add_constant(X)\n",
    "model2 = sm.OLS(y, X_with_intercept).fit()\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " the durbin watson score shows that there my be some autocorrelation present in the data, but it is very close to 2, which is the cut off that starts the non-autocorrelation spec(2-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### same as above dropping sqm_lot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = price_log\n",
    "X = df_dummies.copy()\n",
    "X.drop(['price','sqm_lot'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "X_with_intercept = sm.add_constant(X)\n",
    "model2_6 = sm.OLS(y, X_with_intercept).fit()\n",
    "model2_6.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# final model assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using model with no scaling,   with target = price/10000, log transformed price, dropping sqm_lot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#log price of price/1000\n",
    "y3 = price_tenth_log\n",
    "X3 = df_dummies.copy()\n",
    "X3.drop(['price', 'sqm_lot', 'zipcode'], axis=1, inplace=True)\n",
    "X_intercept = sm.add_constant(X3)\n",
    "model_10 = sm.OLS(y3, X_intercept).fit()\n",
    "model_10.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## y = log transformed price(og)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y as  log of y\n",
    "y = price_log\n",
    "X = df_dummies.copy()\n",
    "X.drop(['price', 'sqm_lot'], axis=1, inplace=True)\n",
    "# create model\n",
    "X_with_intercept = sm.add_constant(X)\n",
    "model = sm.OLS(y, X_with_intercept).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "regression = LinearRegression()\n",
    "\n",
    "crossvalidation = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "baseline = np.mean(cross_val_score(regression, X, y, scoring='r2', cv=crossvalidation))\n",
    "baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "combinations = list(combinations(X, 2))\n",
    "\n",
    "interactions = []\n",
    "data_X = X.copy()\n",
    "for comb in combinations:\n",
    "    data['interaction'] = data[comb[0]] * data[comb[1]]\n",
    "    score = np.mean(cross_val_score(regression, data, y, scoring='r2', cv=crossvalidation))\n",
    "    if score > baseline: interactions.append((comb[0], comb[1], round(score, 3)))\n",
    "            \n",
    "print('Top 7 interactions: %s' %sorted(interactions, key=lambda inter: inter[2], reverse=True)[:7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inter = data.copy()\n",
    "df_inter = df_inter.drop(['price'])\n",
    "ls_interactions = sorted(interactions, key=lambda inter: inter[2], reverse=True)[:7]\n",
    "for inter in ls_interactions:\n",
    "    df_inter[inter[0] + '_' + inter[1]] = X[inter[0]] * X[inter[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "polynomials = []\n",
    "for col in df_inter.columns:\n",
    "    for degree in [2, 3, 4]:\n",
    "        X_data = X.copy()\n",
    "        poly = PolynomialFeatures(degree, include_bias=False)\n",
    "        X_transformed = poly.fit_transform(X_data[[col]])\n",
    "        data = pd.concat([data.drop(col, axis=1),pd.DataFrame(X_transformed)], axis=1)\n",
    "        score = np.mean(cross_val_score(regression, X_data, y, scoring='r2', cv=crossvalidation))\n",
    "        if score > baseline: polynomials.append((col, degree, round(score, 3)))\n",
    "print('Top 10 polynomials: %s' %sorted(polynomials, key=lambda poly: poly[2], reverse=True)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynom = pd.DataFrame(polynomials)\n",
    "polynom.groupby([0], sort=False)[2].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['sqm_living', 'sqm_living15', 'distance_seattle']:\n",
    "    poly = PolynomialFeatures(4, include_bias=False)\n",
    "    X_transformed = poly.fit_transform(X[[col]])\n",
    "    colnames= [col, col + '_' + '2',  col + '_' + '3', col + '_' + '4']\n",
    "    df_inter = pd.concat([df_inter.drop(col, axis=1), pd.DataFrame(X_transformed, columns=colnames)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model = np.mean(cross_val_score(regression, df_inter, y, scoring='r2', cv=crossvalidation))\n",
    "full_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y as  log of y\n",
    "y = price_log\n",
    "X = df_inter.copy()\n",
    "X.drop(['price', 'sqm_lot'], axis=1, inplace=True)\n",
    "# create model\n",
    "X_with_intercept = sm.add_constant(X)\n",
    "model = sm.OLS(y, X_with_intercept).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model with price /10000 log transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model_10 = LinearRegression(normalize=True)\n",
    "my_model_10.fit(X3, y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossvalidation = KFold(n_splits=100, shuffle=True, random_state=1)\n",
    "baseline = np.mean(cross_val_score(my_model_10, X3, y3, scoring='r2', cv=crossvalidation))\n",
    "\n",
    "baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_10 = my_model_10.predict(X3)\n",
    "y_hat_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get coefficients\n",
    "coef_10 = my_model_10.coef_.reshape(-1,1)\n",
    "# back translate coefficients\n",
    "coef_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(\n",
    "            my_model_10, \n",
    "            X3,\n",
    "            y3,\n",
    "            cv=8,\n",
    "            scoring=\"neg_mean_squared_error\"\n",
    ")\n",
    "\n",
    "rmse_scores = np.sqrt(-scores)\n",
    "display(rmse_scores)\n",
    "\n",
    "print(f'Mean: {rmse_scores.mean()}')\n",
    "print(f'Std Dev: {rmse_scores.std()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stepwise_selection(X3, y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model with columns from stepwise selection\n",
    "X3_sw = X[['grade_7',\n",
    " 'grade_11',\n",
    " 'grade_10',\n",
    " 'grade_9',\n",
    " 'distance_seattle',\n",
    " 'grade_12',\n",
    " 'grade_8',\n",
    " 'age',\n",
    " 'sqm_living15',\n",
    " 'sqm_living',\n",
    " 'condition_5',\n",
    " 'grade_13',\n",
    " 'condition_4',\n",
    " 'bathrooms_4.0',\n",
    " 'bathrooms_3.0',\n",
    " 'bathrooms_2.0',\n",
    " 'bed_2',\n",
    " 'bathrooms_5.0',\n",
    " 'condition_3',\n",
    " 'bed_3',\n",
    " 'bathrooms_6.0',\n",
    " 'bed_7',\n",
    " 'bed_6',\n",
    "]]\n",
    "y3 = price_tenth_log\n",
    "X_with_intercept = sm.add_constant(X3_sw)\n",
    "sw3_model = sm.OLS(y3, X_with_intercept).fit()\n",
    "sw3_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model with price log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = LinearRegression()\n",
    "my_model.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossvalidation = KFold(n_splits=100, shuffle=True, random_state=1)\n",
    "baseline = np.mean(cross_val_score(my_model, X, y, scoring='r2', cv=crossvalidation))\n",
    "\n",
    "baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get coefficients\n",
    "coef = my_model.coef_.reshape(-1,1)\n",
    "coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = my_model.predict(X)\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(\n",
    "            my_model, \n",
    "            X,\n",
    "            y,\n",
    "            cv=8,\n",
    "            scoring=\"neg_mean_squared_error\"\n",
    ")\n",
    "\n",
    "rmse_scores = np.sqrt(-scores)\n",
    "display(rmse_scores)\n",
    "\n",
    "print(f'Mean: {rmse_scores.mean()}')\n",
    "print(f'Std Dev: {rmse_scores.std()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stepwise_selection(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### create model using variables dictated by stepwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model with columns from stepwise selection\n",
    "X_sw = X[['distance_seattle',\n",
    " 'grade_7',\n",
    " 'grade_10',\n",
    " 'grade_11',\n",
    " 'grade_9',\n",
    " 'grade_12',\n",
    " 'sqm_living',\n",
    " 'grade_8',\n",
    " 'age',\n",
    " 'sqm_living15',\n",
    " 'condition_5',\n",
    " 'grade_13',\n",
    " 'condition_4',\n",
    " 'bathrooms_4.0',\n",
    " 'bathrooms_3.0',\n",
    " 'bathrooms_2.0',\n",
    " 'bed_2',\n",
    " 'bathrooms_5.0',\n",
    " 'condition_3',\n",
    " 'bed_3',\n",
    " 'bathrooms_6.0',\n",
    " 'bed_7',\n",
    " 'bed_6',\n",
    "]]\n",
    "y = price_log\n",
    "X_with_intercept = sm.add_constant(X_sw)\n",
    "sw_model = sm.OLS(y, X_with_intercept).fit()\n",
    "sw_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "table = sm.stats.anova_lm(sw_model, typ=2)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the skew  looks to prove a fairly normal distribution, but kurtosis shows slightly heavier tails than those found in a normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confidence intervals\n",
    "sw_model.conf_int()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## breuschpagan test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm, lm_pval, f_val, f_pval = diagnostic.het_breuschpagan(sw_model.resid, sw_model.model.exog)\n",
    "print('p-value of lagrange multiplier test {:.3}'.format(lm_pval))\n",
    "print('the f value is {:.3}'.format(f_val))\n",
    "print('the f p-value is {:.3}'.format(f_pval))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The null hypothesis assumes homoskedasticity. for p_val > 0.05, fail to reject the null and conclude there may not be heteroskedasticity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylab\n",
    "sm.qqplot(sw_model.resid, line = 's')\n",
    "pylab.show()\n",
    "\n",
    "\n",
    "mean_resid = sum(sw_model.resid)/len(sw_model.resid)\n",
    "mean_resid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mean residual indicates that predicted prices are higher than actual prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = sw_model.predict()\n",
    "print(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse =np.sqrt(mean_squared_error(y, y_hat))\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SS_Residual = np.sum((y-y_hat)**2)\n",
    "SS_Total = np.sum((y-np.mean(y))**2)\n",
    "r_squared = 1 - (float(SS_Residual))/SS_Total\n",
    "adjusted_r_squared = 1 - (1-r_squared)*(len(y)-1)/(len(y)-X_sw.shape[1]-1)\n",
    "print(SS_Residual)\n",
    "print(SS_Total)\n",
    "print(r_squared)\n",
    "print(adjusted_r_squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparison graph with log price \n",
    "ax1 = sns.distplot(y, hist=False, color=\"r\", label=\"Actual\")\n",
    "sns.distplot(y_hat, hist=False, color=\"b\", label=\"Predicted\" , ax=ax1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_model.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_params_exp = np.exp(sw_model.params)\n",
    "sw_params_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparison graph with log price translated to original format\n",
    "ax1 = sns.distplot(np.exp(y), hist=False, color=\"r\", label=\"Actual\")\n",
    "sns.distplot(np.exp(y_hat), hist=False, color=\"b\", label=\"Predicted\" , ax=ax1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating model with scikit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_sw, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(\n",
    "            lr_model, \n",
    "            X_sw,\n",
    "            y,\n",
    "            cv=8,\n",
    "            scoring=\"neg_mean_squared_error\"\n",
    ")\n",
    "\n",
    "rmse_scores = np.sqrt(-scores)\n",
    "display(rmse_scores)\n",
    "\n",
    "print(f'Mean: {rmse_scores.mean()}')\n",
    "print(f'Std Dev: {rmse_scores.std()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform cross validation on  model\n",
    "crossvalidation = KFold(n_splits=10, shuffle=True, random_state=23\n",
    "                       )\n",
    "baseline = np.mean(cross_val_score(lr_model, X_sw, y, scoring='r2', cv=crossvalidation))\n",
    "\n",
    "baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get intercept and coef\n",
    "print('The intercept for model is {:.4}'.format(lr_model.intercept_))\n",
    "\n",
    "\n",
    "#loop through dict to print\n",
    "for cf in zip(X_sw.columns, lr_model.coef_):\n",
    "    print('The coefficient for {} is {:.4}'.format(cf[0], cf[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train/test split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_sw, y, test_size=0.3, random_state=29)\n",
    "print(X_train.shape); print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model = LinearRegression()\n",
    "train_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run model with trianing data\n",
    "pred_train_lr= train_model.predict(X_train)\n",
    "print(np.sqrt(mean_squared_error(y_train,pred_train_lr)))\n",
    "print(r2_score(y_train, pred_train_lr))\n",
    "\n",
    "\n",
    "# run model with test data \n",
    "pred_test_lr= train_model.predict(X_test)\n",
    "print(np.sqrt(mean_squared_error(y_test,pred_test_lr))) \n",
    "print(r2_score(y_test, pred_test_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_train = train_model.predict(X_train)\n",
    "predict_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_test = train_model.predict(X_test)\n",
    "predict_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get intercept and coef\n",
    "ax1 = sns.distplot(y, hist=False, color=\"r\", label=\"Actual\")\n",
    "sns.distplot(predict_train, hist=False, color=\"b\", label=\"Predicted\" , ax=ax1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax1 = sns.distplot(y, hist=False, color=\"r\", label=\"Actual\")\n",
    "sns.distplot(predict_test, hist=False, color=\"b\", label=\"Predicted\" , ax=ax1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualize training data vs test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "ax1 = sns.distplot(predict_train, hist=False, color=\"r\", label=\"Train\")\n",
    "sns.distplot(predict_test, hist=False, color=\"b\", label=\"Test\" , ax=ax1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model.coef_.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossvalidation = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "baseline = np.mean(cross_val_score(train_model, X_train, y_train, scoring='r2', cv=crossvalidation))\n",
    "\n",
    "baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparison graph with log price \n",
    "ax1 = sns.distplot(y, hist=False, color=\"r\", label=\"Actual\")\n",
    "sns.distplot(y_hat, hist=False, color=\"b\", label=\"Predicted\" , ax=ax1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = train_model.predict(X_sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid = y_hat - y\n",
    "resid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get intercept and coef for training model\n",
    "intercept = np.exp(train_model.intercept_)\n",
    "print('The intercept for model is {:.7}'.format(intercept))\n",
    "\n",
    "\n",
    "#loop through dict to print\n",
    "for cf in zip(X_train.columns, np.exp(train_model.coef_)):\n",
    "    print('The coefficient for {} is {:.4}'.format(cf[0], cf[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get intercept and coef for training model\n",
    "print('The intercept for model is {:.4}'.format(train_model.intercept_))\n",
    "\n",
    "\n",
    "#loop through dict to print\n",
    "for cf in zip(X_train.columns,(train_model.coef_)):\n",
    "    print('The coefficient for {} is {:.4}'.format(cf[0], cf[1]))\n",
    "    print('With other variables remaining constant, for every 1 unit  change in {}, there is a change in log of y for X by {:.2} units ' .format(cf[0], cf[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.concat([X_sw, y,], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "#pickle model\n",
    "pickle.dump(train_model, open('final_king_county_lm_pickle.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the data used for regression models is king county washington housing dataset, provided by flatiron school, original available through https://data.kingcounty.gov/ \n",
    "additional versions of dataset available through kaggle.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reject null hypothesis. \n",
    "fail to reject alternative hypothesis. \n",
    "the features of a house display a correlation with the price/value of a house \n",
    "pvalues do not exceed 0.5\n",
    "model R2 = 0.732\n",
    "mean residual = -1.0649293058159106e-14 which indicates target is predicted as higher value than the actual value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Breusch-Pagan test assumes there may not be heteroskedacity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of bedrooms has direct relationship with target value price\n",
    "Age, when combined with condition and grade has effect on price\n",
    "Condition and grade affect price\n",
    "Distance to Seattle affects price, more expensive properties tend to occur in the central distance from Seattle area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering and Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. distance from Seattle: \n",
    "attributes used to create feature:\n",
    "lat, long \n",
    "additional data required, obtained lat, long for city of Seattle\n",
    "2. age of house:\n",
    "attributes used to create feature:\n",
    "yr_built, date\n",
    "3. square meter living area:\n",
    "attributes used to create feature:\n",
    "sqft_living, sqft_living15\n",
    "feature selection from original dataset:\n",
    "bedrooms, bathrooms, grade, condition\n",
    "(sqft_living, sqft_living15, yr_built, date, lat, long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dummy variables used for categorical data variables:\n",
    "bedrooms, bathrooms, grade, condition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future Assessments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assess possible standard scaler use on continuous data of same unit of measurement. assess other continuous data to determine scaling and normalizing possibilties to improve model accuracy. \n",
    "Assess polynomial regression for variables.\n",
    "Assess need for weighted variables with more influence on price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Final-Project-Submission\" data-toc-modified-id=\"Final-Project-Submission-0.1\"><span class=\"toc-item-num\">0.1&nbsp;&nbsp;</span>Final Project Submission</a></span></li></ul></li><li><span><a href=\"#data-scrubbing-and-cleaning\" data-toc-modified-id=\"data-scrubbing-and-cleaning-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>data scrubbing and cleaning</a></span><ul class=\"toc-item\"><li><span><a href=\"#import-modules-and-data\" data-toc-modified-id=\"import-modules-and-data-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>import modules and data</a></span></li><li><span><a href=\"#functions\" data-toc-modified-id=\"functions-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>functions</a></span></li><li><span><a href=\"#create-series-to-present-price-in-readable-way-for-visualizations\" data-toc-modified-id=\"create-series-to-present-price-in-readable-way-for-visualizations-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>create series to present price in readable way for visualizations</a></span></li><li><span><a href=\"#fill-in-missing-values-(NaN)-where-applicable\" data-toc-modified-id=\"fill-in-missing-values-(NaN)-where-applicable-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>fill in missing values (NaN) where applicable</a></span></li><li><span><a href=\"#assess-whether-renovation-has-direct-impact-on-home-price\" data-toc-modified-id=\"assess-whether-renovation-has-direct-impact-on-home-price-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>assess whether renovation has direct impact on home price</a></span></li><li><span><a href=\"#convert-date-column-to-DateTime-format,-then-separate-date-to-month,-day,-year-columns-due-to-datetime-being-unusable-in-regression,-(drop-'data'-column-once-all-necessary-calculations-have-been-done)\" data-toc-modified-id=\"convert-date-column-to-DateTime-format,-then-separate-date-to-month,-day,-year-columns-due-to-datetime-being-unusable-in-regression,-(drop-'data'-column-once-all-necessary-calculations-have-been-done)-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>convert date column to DateTime format, then separate date to month, day, year columns due to datetime being unusable in regression, (drop 'data' column once all necessary calculations have been done)</a></span></li><li><span><a href=\"#create-age-column-from-date-of-sale-and-year-built\" data-toc-modified-id=\"create-age-column-from-date-of-sale-and-year-built-1.7\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>create age column from date of sale and year built</a></span></li><li><span><a href=\"#find-out-if-there-are-historic-homes-on-dataset.-washington-state-qualifies-as-over-50-and-retaining-original-structure.-assess-whether-these-are-abnormalities\" data-toc-modified-id=\"find-out-if-there-are-historic-homes-on-dataset.-washington-state-qualifies-as-over-50-and-retaining-original-structure.-assess-whether-these-are-abnormalities-1.8\"><span class=\"toc-item-num\">1.8&nbsp;&nbsp;</span>find out if there are historic homes on dataset. washington state qualifies as over 50 and retaining original structure. assess whether these are abnormalities</a></span></li><li><span><a href=\"#assess-whether-renovating-a-historic-home-increases-market-value-as-far-as-price\" data-toc-modified-id=\"assess-whether-renovating-a-historic-home-increases-market-value-as-far-as-price-1.9\"><span class=\"toc-item-num\">1.9&nbsp;&nbsp;</span>assess whether renovating a historic home increases market value as far as price</a></span><ul class=\"toc-item\"><li><span><a href=\"#drop-columns-not-to-be-utilized\" data-toc-modified-id=\"drop-columns-not-to-be-utilized-1.9.1\"><span class=\"toc-item-num\">1.9.1&nbsp;&nbsp;</span>drop columns not to be utilized</a></span></li></ul></li><li><span><a href=\"#sqft_basement-column-contains-string-characters,-remove-these-and-convert-to-int-to-match-other-similar-columns\" data-toc-modified-id=\"sqft_basement-column-contains-string-characters,-remove-these-and-convert-to-int-to-match-other-similar-columns-1.10\"><span class=\"toc-item-num\">1.10&nbsp;&nbsp;</span>sqft_basement column contains string characters, remove these and convert to int to match other similar columns</a></span></li><li><span><a href=\"#convert-sq_ft-areas-to-square-meters\" data-toc-modified-id=\"convert-sq_ft-areas-to-square-meters-1.11\"><span class=\"toc-item-num\">1.11&nbsp;&nbsp;</span>convert sq_ft areas to square meters</a></span></li><li><span><a href=\"#bedrooms-outlier-present-as-33-bedroom-house,-appears-to-be-typo-due-to-other-info,-should-be-3-bedrooms-as-this-listing-only-has-1.75-bathrooms\" data-toc-modified-id=\"bedrooms-outlier-present-as-33-bedroom-house,-appears-to-be-typo-due-to-other-info,-should-be-3-bedrooms-as-this-listing-only-has-1.75-bathrooms-1.12\"><span class=\"toc-item-num\">1.12&nbsp;&nbsp;</span>bedrooms outlier present as 33 bedroom house, appears to be typo due to other info, should be 3 bedrooms as this listing only has 1.75 bathrooms</a></span></li><li><span><a href=\"#assessing-grade-category\" data-toc-modified-id=\"assessing-grade-category-1.13\"><span class=\"toc-item-num\">1.13&nbsp;&nbsp;</span>assessing grade category</a></span><ul class=\"toc-item\"><li><span><a href=\"#assess-the-affect-that-the-grade-of-the-house-hs-on-the-sale-value\" data-toc-modified-id=\"assess-the-affect-that-the-grade-of-the-house-hs-on-the-sale-value-1.13.1\"><span class=\"toc-item-num\">1.13.1&nbsp;&nbsp;</span>assess the affect that the grade of the house hs on the sale value</a></span></li><li><span><a href=\"#assess-condition-category\" data-toc-modified-id=\"assess-condition-category-1.13.2\"><span class=\"toc-item-num\">1.13.2&nbsp;&nbsp;</span>assess condition category</a></span></li><li><span><a href=\"#assess-lot-size-and-home-type\" data-toc-modified-id=\"assess-lot-size-and-home-type-1.13.3\"><span class=\"toc-item-num\">1.13.3&nbsp;&nbsp;</span>assess lot size and home type</a></span></li></ul></li><li><span><a href=\"#assess-for-duplicates\" data-toc-modified-id=\"assess-for-duplicates-1.14\"><span class=\"toc-item-num\">1.14&nbsp;&nbsp;</span>assess for duplicates</a></span></li><li><span><a href=\"#evaluate-duplicate-ids\" data-toc-modified-id=\"evaluate-duplicate-ids-1.15\"><span class=\"toc-item-num\">1.15&nbsp;&nbsp;</span>evaluate duplicate ids</a></span></li><li><span><a href=\"#use-latitude-and-longitude-in-existing-dataset-to-get-the-distance-from-home-to-Seattle(closest-major-city)\" data-toc-modified-id=\"use-latitude-and-longitude-in-existing-dataset-to-get-the-distance-from-home-to-Seattle(closest-major-city)-1.16\"><span class=\"toc-item-num\">1.16&nbsp;&nbsp;</span>use latitude and longitude in existing dataset to get the distance from home to Seattle(closest major city)</a></span><ul class=\"toc-item\"><li><span><a href=\"#import-geopy-modules\" data-toc-modified-id=\"import-geopy-modules-1.16.1\"><span class=\"toc-item-num\">1.16.1&nbsp;&nbsp;</span>import geopy modules</a></span></li><li><span><a href=\"#use-geopy-to-set-up-points-and-calculate-distance\" data-toc-modified-id=\"use-geopy-to-set-up-points-and-calculate-distance-1.16.2\"><span class=\"toc-item-num\">1.16.2&nbsp;&nbsp;</span>use geopy to set up points and calculate distance</a></span></li><li><span><a href=\"#assess-data-by-zipcode\" data-toc-modified-id=\"assess-data-by-zipcode-1.16.3\"><span class=\"toc-item-num\">1.16.3&nbsp;&nbsp;</span>assess data by zipcode</a></span></li><li><span><a href=\"#dropping-columns\" data-toc-modified-id=\"dropping-columns-1.16.4\"><span class=\"toc-item-num\">1.16.4&nbsp;&nbsp;</span>dropping columns</a></span></li></ul></li><li><span><a href=\"#dropping-all-sqft-columns-due-to-conversion-to-metric-system\" data-toc-modified-id=\"dropping-all-sqft-columns-due-to-conversion-to-metric-system-1.17\"><span class=\"toc-item-num\">1.17&nbsp;&nbsp;</span>dropping all sqft columns due to conversion to metric system</a></span></li><li><span><a href=\"#create-id-dataframe-to-attach-once-model-is-complete.-remove-id-from-dataset-as-it-isn't-relevant-but-just-identifies-homes\" data-toc-modified-id=\"create-id-dataframe-to-attach-once-model-is-complete.-remove-id-from-dataset-as-it-isn't-relevant-but-just-identifies-homes-1.18\"><span class=\"toc-item-num\">1.18&nbsp;&nbsp;</span>create id dataframe to attach once model is complete. remove id from dataset as it isn't relevant but just identifies homes</a></span></li></ul></li><li><span><a href=\"#check-for-multicollinearity\" data-toc-modified-id=\"check-for-multicollinearity-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>check for multicollinearity</a></span><ul class=\"toc-item\"><li><span><a href=\"#heatmap-for-correlation-assessment\" data-toc-modified-id=\"heatmap-for-correlation-assessment-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>heatmap for correlation assessment</a></span></li><li><span><a href=\"#create-new-dataframe-to-further-assess-correlation\" data-toc-modified-id=\"create-new-dataframe-to-further-assess-correlation-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>create new dataframe to further assess correlation</a></span></li><li><span><a href=\"#scatter-matrix-to-further-assess-multicollinearity\" data-toc-modified-id=\"scatter-matrix-to-further-assess-multicollinearity-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>scatter matrix to further assess multicollinearity</a></span><ul class=\"toc-item\"><li><span><a href=\"#assess-for-outliers\" data-toc-modified-id=\"assess-for-outliers-2.3.1\"><span class=\"toc-item-num\">2.3.1&nbsp;&nbsp;</span>assess for outliers</a></span></li></ul></li></ul></li><li><span><a href=\"#data-exploration\" data-toc-modified-id=\"data-exploration-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>data exploration</a></span><ul class=\"toc-item\"><li><span><a href=\"#histograms\" data-toc-modified-id=\"histograms-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>histograms</a></span></li><li><span><a href=\"#pairplot\" data-toc-modified-id=\"pairplot-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>pairplot</a></span></li><li><span><a href=\"#kde\" data-toc-modified-id=\"kde-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>kde</a></span></li><li><span><a href=\"#check-price-skew,-visualize-tranformed-data,-normalize-price-using-log-transformation\" data-toc-modified-id=\"check-price-skew,-visualize-tranformed-data,-normalize-price-using-log-transformation-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>check price skew, visualize tranformed data, normalize price using log transformation</a></span></li><li><span><a href=\"#regression-plots-with-transformed-data\" data-toc-modified-id=\"regression-plots-with-transformed-data-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>regression plots with transformed data</a></span></li><li><span><a href=\"#age/price-scatter\" data-toc-modified-id=\"age/price-scatter-3.6\"><span class=\"toc-item-num\">3.6&nbsp;&nbsp;</span>age/price scatter</a></span></li><li><span><a href=\"#distance-to-seattle/price-scatter\" data-toc-modified-id=\"distance-to-seattle/price-scatter-3.7\"><span class=\"toc-item-num\">3.7&nbsp;&nbsp;</span>distance to seattle/price scatter</a></span></li><li><span><a href=\"#visualize-categorical-variables-by-price\" data-toc-modified-id=\"visualize-categorical-variables-by-price-3.8\"><span class=\"toc-item-num\">3.8&nbsp;&nbsp;</span>visualize categorical variables by price</a></span><ul class=\"toc-item\"><li><span><a href=\"#comparing-grade-and-condition-columns-visually\" data-toc-modified-id=\"comparing-grade-and-condition-columns-visually-3.8.1\"><span class=\"toc-item-num\">3.8.1&nbsp;&nbsp;</span>comparing grade and condition columns visually</a></span></li></ul></li></ul></li><li><span><a href=\"#one-hot-encoding,-create-new-df:-dummies\" data-toc-modified-id=\"one-hot-encoding,-create-new-df:-dummies-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>one hot encoding, create new df: dummies</a></span><ul class=\"toc-item\"><li><span><a href=\"#create-dummy-df\" data-toc-modified-id=\"create-dummy-df-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>create dummy df</a></span></li></ul></li><li><span><a href=\"#models\" data-toc-modified-id=\"models-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>models</a></span><ul class=\"toc-item\"><li><span><a href=\"#import-libraries\" data-toc-modified-id=\"import-libraries-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>import libraries</a></span><ul class=\"toc-item\"><li><span><a href=\"#no-dummies-dataframe\" data-toc-modified-id=\"no-dummies-dataframe-5.1.1\"><span class=\"toc-item-num\">5.1.1&nbsp;&nbsp;</span>no dummies dataframe</a></span></li><li><span><a href=\"#df-with-dummies\" data-toc-modified-id=\"df-with-dummies-5.1.2\"><span class=\"toc-item-num\">5.1.2&nbsp;&nbsp;</span>df with dummies</a></span></li></ul></li></ul></li><li><span><a href=\"#single-variable-linear-regression-on-data-with-non-transformed-price\" data-toc-modified-id=\"single-variable-linear-regression-on-data-with-non-transformed-price-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>single variable linear regression on data with non transformed price</a></span></li><li><span><a href=\"#single-variable-linear-regression-on-data-with-log-transformed-price\" data-toc-modified-id=\"single-variable-linear-regression-on-data-with-log-transformed-price-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>single variable linear regression on data with log transformed price</a></span><ul class=\"toc-item\"><li><span><a href=\"#assessing-grade-affect-on-price,-converting-grade-to-grade/13-as-there-are-only-13-possible-grades\" data-toc-modified-id=\"assessing-grade-affect-on-price,-converting-grade-to-grade/13-as-there-are-only-13-possible-grades-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>assessing grade affect on price, converting grade to grade/13 as there are only 13 possible grades</a></span></li></ul></li><li><span><a href=\"#build-model-with-df\" data-toc-modified-id=\"build-model-with-df-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>build model with df</a></span><ul class=\"toc-item\"><li><span><a href=\"#where-X=-data,-with-dummies,-y-=price\" data-toc-modified-id=\"where-X=-data,-with-dummies,-y-=price-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span>where X= data, with dummies, y =price</a></span></li><li><span><a href=\"#where-X=data,-y-=-price,-no-dummies\" data-toc-modified-id=\"where-X=data,-y-=-price,-no-dummies-8.2\"><span class=\"toc-item-num\">8.2&nbsp;&nbsp;</span>where X=data, y = price, no dummies</a></span></li><li><span><a href=\"#regression-on-datasets-with-dummies\" data-toc-modified-id=\"regression-on-datasets-with-dummies-8.3\"><span class=\"toc-item-num\">8.3&nbsp;&nbsp;</span>regression on datasets with dummies</a></span></li><li><span><a href=\"#where-X=-minmax-scaling-applied-to-dataset,-y=log-transformed-price\" data-toc-modified-id=\"where-X=-minmax-scaling-applied-to-dataset,-y=log-transformed-price-8.4\"><span class=\"toc-item-num\">8.4&nbsp;&nbsp;</span>where X= minmax scaling applied to dataset, y=log transformed price</a></span></li><li><span><a href=\"#model-where-X=-data,-no-scaling-applied-to-continuous/y-=-log-transformed-price\" data-toc-modified-id=\"model-where-X=-data,-no-scaling-applied-to-continuous/y-=-log-transformed-price-8.5\"><span class=\"toc-item-num\">8.5&nbsp;&nbsp;</span>model where X= data, no scaling applied to continuous/y = log transformed price</a></span><ul class=\"toc-item\"><li><span><a href=\"#same-as-above-dropping-sqm_lot\" data-toc-modified-id=\"same-as-above-dropping-sqm_lot-8.5.1\"><span class=\"toc-item-num\">8.5.1&nbsp;&nbsp;</span>same as above dropping sqm_lot</a></span></li></ul></li></ul></li><li><span><a href=\"#final-model-assessment\" data-toc-modified-id=\"final-model-assessment-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>final model assessment</a></span><ul class=\"toc-item\"><li><span><a href=\"#using-model-with-no-scaling,---with-target-=-price/10000,-log-transformed-price,-dropping-sqm_lot\" data-toc-modified-id=\"using-model-with-no-scaling,---with-target-=-price/10000,-log-transformed-price,-dropping-sqm_lot-9.1\"><span class=\"toc-item-num\">9.1&nbsp;&nbsp;</span>using model with no scaling,   with target = price/10000, log transformed price, dropping sqm_lot</a></span></li><li><span><a href=\"#y-=-log-transformed-price(og)\" data-toc-modified-id=\"y-=-log-transformed-price(og)-9.2\"><span class=\"toc-item-num\">9.2&nbsp;&nbsp;</span>y = log transformed price(og)</a></span></li><li><span><a href=\"#model-with-price-/10000-log-transformed\" data-toc-modified-id=\"model-with-price-/10000-log-transformed-9.3\"><span class=\"toc-item-num\">9.3&nbsp;&nbsp;</span>model with price /10000 log transformed</a></span></li><li><span><a href=\"#model-with-price-log\" data-toc-modified-id=\"model-with-price-log-9.4\"><span class=\"toc-item-num\">9.4&nbsp;&nbsp;</span>model with price log</a></span></li><li><span><a href=\"#breuschpagan-test\" data-toc-modified-id=\"breuschpagan-test-9.5\"><span class=\"toc-item-num\">9.5&nbsp;&nbsp;</span>breuschpagan test</a></span></li><li><span><a href=\"#residuals\" data-toc-modified-id=\"residuals-9.6\"><span class=\"toc-item-num\">9.6&nbsp;&nbsp;</span>residuals</a></span></li><li><span><a href=\"#train/test-split-data\" data-toc-modified-id=\"train/test-split-data-9.7\"><span class=\"toc-item-num\">9.7&nbsp;&nbsp;</span>train/test split data</a></span><ul class=\"toc-item\"><li><span><a href=\"#visualize-training-data-vs-test-data\" data-toc-modified-id=\"visualize-training-data-vs-test-data-9.7.1\"><span class=\"toc-item-num\">9.7.1&nbsp;&nbsp;</span>visualize training data vs test data</a></span></li></ul></li></ul></li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Conclusion</a></span><ul class=\"toc-item\"><li><span><a href=\"#Feature-Engineering-and-Feature-Selection\" data-toc-modified-id=\"Feature-Engineering-and-Feature-Selection-10.1\"><span class=\"toc-item-num\">10.1&nbsp;&nbsp;</span>Feature Engineering and Feature Selection</a></span></li></ul></li><li><span><a href=\"#Future-Assessments\" data-toc-modified-id=\"Future-Assessments-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;</span>Future Assessments</a></span></li></ul></div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "174px",
    "width": "329px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
